{
    "nbformat_minor": 1, 
    "cells": [
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 38, 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 39, 
            "source": "from io import BytesIO  \nimport requests  \nimport json  \nimport pandas as pd\nfrom io import BytesIO  \nimport requests  \nimport json  \nimport pandas as pd\n\ndef put_file(credentials, local_file_name):  \n    \"\"\"This functions returns a StringIO object containing\n    the file content from Bluemix Object Storage V3.\"\"\"\n    f = open(local_file_name,'r')\n    my_data = f.read()\n    url1 = ''.join(['https://identity.open.softlayer.com', '/v3/auth/tokens'])\n    data = {'auth': {'identity': {'methods': ['password'],\n            'password': {'user': {'name': credentials['username'],'domain': {'id': credentials['domain_id']},\n            'password': credentials['password']}}}}}\n    headers1 = {'Content-Type': 'application/json'}\n    resp1 = requests.post(url=url1, data=json.dumps(data), headers=headers1)\n    resp1_body = resp1.json()\n    for e1 in resp1_body['token']['catalog']:\n        if(e1['type']=='object-store'):\n            for e2 in e1['endpoints']:\n                        if(e2['interface']=='public'and e2['region']=='dallas'):\n                            url2 = ''.join([e2['url'],'/', credentials['container'], '/', local_file_name])\n    s_subject_token = resp1.headers['x-subject-token']\n    headers2 = {'X-Auth-Token': s_subject_token, 'accept': 'application/json'}\n    resp2 = requests.put(url=url2, headers=headers2, data = my_data )\n    print resp2"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 40, 
            "source": "import numpy as np\nimport pandas as pd\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import LabelEncoder\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, make_scorer"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 41, 
            "source": "train = pd.read_csv(get_object_storage_file_with_credentials_9b3c597dc9074fdc8a1dbb2f44d73d63('DefaultProjectlakshmi1310nagagmailcom', 'train.csv'))\ntest = pd.read_csv(get_object_storage_file_with_credentials_9b3c597dc9074fdc8a1dbb2f44d73d63('DefaultProjectlakshmi1310nagagmailcom', 'test.csv'))"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "User_ID         0\nDescription     0\nBrowser_Used    0\nDevice_Used     0\nIs_Response     0\ndtype: int64\nhappy        0.681213\nnot happy    0.318787\nName: Is_Response, dtype: float64\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 42, 
            "source": "print train.isnull().sum()\nprint train.Is_Response.value_counts().apply(lambda x:x*1./len(train))"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Above values show that data set is imbalance problem and has no null values"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Creating xgboost model with only 2 features "
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 44, 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 45, 
            "source": "# function to clean data\n\nstops = set(stopwords.words(\"english\"))\ndef cleanData(text, lowercase = False, remove_stops = False, stemming = False):\n    txt = str(text)\n    txt = re.sub(r'[^A-Za-z0-9\\s]',r'',txt)\n    txt = re.sub(r'\\n',r' ',txt)\n    \n    if lowercase:\n        txt = \" \".join([w.lower() for w in txt.split()])\n        \n    if remove_stops:\n        txt = \" \".join([w for w in txt.split() if w not in stops])\n    \n    if stemming:\n        st = PorterStemmer()\n        txt = \" \".join([st.stem(w) for w in txt.split()])\n\n    return txt"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "(68336, 5)\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 46, 
            "source": "test['Is_Response'] = np.nan\ndf = train.append(test)\nprint(df.shape)"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 47, 
            "source": "# clean description\ndf['Description'] = df['Description'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=True))"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 48, 
            "source": "\n\n# initialise the functions - we'll create separate models for each type.\ncountvec = CountVectorizer(analyzer='word', ngram_range = (1,1), min_df=150, max_features=500)\ntfidfvec = TfidfVectorizer(analyzer='word', ngram_range = (1,1), min_df = 150, max_features=500)\n\n"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 49, 
            "source": "bagofwords = countvec.fit_transform(df['Description'])\ntfidfdata = tfidfvec.fit_transform(df['Description'])"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 50, 
            "source": "# label encode categorical features in data given\ncols = ['Browser_Used','Device_Used']\n\nfor x in cols:\n    lbl = LabelEncoder()\n    df[x] = lbl.fit_transform(df[x])"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 51, 
            "source": "# create dataframe for features\nbow_df = pd.DataFrame(bagofwords.todense())\ntfidf_df = pd.DataFrame(tfidfdata.todense())\n"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 52, 
            "source": "# set column names\nbow_df.columns = ['col'+ str(x) for x in bow_df.columns]\ntfidf_df.columns = ['col' + str(x) for x in tfidf_df.columns]\n"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 53, 
            "source": "# create separate data frame for bag of words and tf-idf\n\nbow_df_train = bow_df[:len(train)]\nbow_df_test = bow_df[len(train):]\n\ntfid_df_train = tfidf_df[:len(train)]\ntfid_df_test = tfidf_df[len(train):]\n"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 54, 
            "source": "# split the merged data file into train and test respectively\ntrain_feats = df[~pd.isnull(df.Is_Response)]\ntest_feats = df[pd.isnull(df.Is_Response)]"
        }, 
        {
            "outputs": [
                {
                    "name": "stderr", 
                    "output_type": "stream", 
                    "text": "/usr/local/src/bluemix_jupyter_bundle.v59/notebook/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  if __name__ == '__main__':\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 55, 
            "source": "train_feats['Is_Response'] = [1 if x == 'happy' else 0 for x in train_feats['Is_Response']]"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 56, 
            "source": "# merge count (bag of word) features into train\nbow_train = pd.concat([train_feats[cols], bow_df_train], axis = 1)\nbow_test = pd.concat([test_feats[cols], bow_df_test], axis=1)\n\nbow_test.reset_index(drop=True, inplace=True)"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 57, 
            "source": "# merge into a new data frame with tf-idf features\ntrain_feats2 = pd.concat([train_feats[cols], tfid_df_train], axis=1)\ntest_feats2 = pd.concat([test_feats[cols], tfid_df_test], axis=1)"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 58, 
            "source": "bow_train.to_csv('bow_train.csv',index=False)\nbow_test.to_csv('bow_test.csv',index=False)\ntrain_feats2.to_csv('tfid_train.csv',index=False)\ntest_feats2.to_csv('tfid_train.csv',index=False)"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "<Response [201]>\n<Response [201]>\n<Response [201]>\n"
                }, 
                {
                    "ename": "IOError", 
                    "output_type": "error", 
                    "evalue": "[Errno 2] No such file or directory: 'tfid_test.csv'", 
                    "traceback": [
                        "\u001b[0;31m\u001b[0m", 
                        "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-59-c580377be000>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mput_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcredentials_10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'bow_test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mput_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcredentials_10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tfid_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mput_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcredentials_10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tfid_test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;32m<ipython-input-39-9125e7c6ea16>\u001b[0m in \u001b[0;36mput_file\u001b[0;34m(credentials, local_file_name)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \"\"\"This functions returns a StringIO object containing\n\u001b[1;32m     12\u001b[0m     the file content from Bluemix Object Storage V3.\"\"\"\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_file_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mmy_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0murl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'https://identity.open.softlayer.com'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/v3/auth/tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'tfid_test.csv'"
                    ]
                }
            ], 
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 59, 
            "source": "put_file(credentials_10,'bow_train.csv')\nput_file(credentials_10,'bow_test.csv')\nput_file(credentials_10,'tfid_train.csv')\nput_file(credentials_10,'tfid_test.csv')"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Data Cleaning part1 done"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<h3>Extracting Details from Description</h3>"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "source": "target=train.Is_Response\ntrain.drop(['Is_Response'],inplace=True,axis=1)\ndf = train.append(test)"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "source": "df=df.assign(description_len = train.Description.str.len())"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "source": "tok_and_tag = lambda x: nltk.pos_tag(nltk.word_tokenize(x.decode('utf-8')))\ndf.loc[:,'tagged_desc']=df['Description'].apply(tok_and_tag)"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "source": "#getting possible tags\ntokens, tags = zip(*chain(*df['tagged_desc'].tolist()))\npossible_tags = sorted(set(tags))\ndf.loc[:,'pos_counts'] = df['tagged_desc'].apply(lambda x: Counter(list(zip(*x))[1]))"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "source": "##Adding zeros to tages not present \ndef add_pos_with_zero_counts(counter, keys_to_add):\n    for k in keys_to_add:\n        counter[k] = counter.get(k, 0)\n    return counter\ndf.loc[:,'pos_counts_with_zero'] =df['pos_counts'].apply(lambda x: add_pos_with_zero_counts(x, possible_tags))"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "source": "df['sent_vector'] =df['pos_counts_with_zero'].apply(lambda x: [count for tag, count in sorted(x.most_common())])"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "source": "dftrain= train.append(test,ignore_index=True)"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "source": "df2 = pd.DataFrame(df['sent_vector'].tolist())\ndf2.columns = possible_tags\ndf2['project_id']=dftrain.project_id"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "source": "nltk_final =pd.merge(df, df2, left_on='project_id', right_on='project_id')"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "source": "put_file(,'FTRL6.csv')"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.0", 
            "name": "python2-spark20", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "version": "2.7.11", 
            "pygments_lexer": "ipython2", 
            "name": "python", 
            "nbconvert_exporter": "python", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }
        }
    }, 
    "nbformat": 4
}